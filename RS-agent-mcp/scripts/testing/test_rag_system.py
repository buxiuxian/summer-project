#!/usr/bin/env python3
"""
RAGå‘é‡æ•°æ®åº“ç³»ç»Ÿæµ‹è¯•è„šæœ¬
"""

import sys
import os
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.rag.knowledge_base import knowledge_manager, add_document_to_knowledge_base

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_embedding_model():
    """æµ‹è¯•åµŒå…¥æ¨¡å‹åŠ è½½"""
    print("ğŸ” æµ‹è¯•1: åµŒå…¥æ¨¡å‹åŠ è½½")
    try:
        model = knowledge_manager._get_embedding_model()
        print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model}")
        print(f"   æ¨¡å‹ç»´åº¦: {model.get_sentence_embedding_dimension()}")
        
        # æµ‹è¯•ç¼–ç åŠŸèƒ½
        test_texts = ["å¾®æ³¢é¥æ„Ÿ", "åœŸå£¤æ¹¿åº¦", "åå‘æ•£å°„ç³»æ•°"]
        embeddings = model.encode(test_texts)
        print(f"   æµ‹è¯•ç¼–ç ç»“æœ: {embeddings.shape}")
        
        return True
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return False

def test_vector_index():
    """æµ‹è¯•å‘é‡ç´¢å¼•"""
    print("\nğŸ” æµ‹è¯•2: å‘é‡ç´¢å¼•çŠ¶æ€")
    try:
        if knowledge_manager.faiss_index is None:
            print("âŒ FAISSç´¢å¼•æœªåˆå§‹åŒ–")
            return False
        
        total_docs = knowledge_manager.faiss_index.ntotal
        print(f"âœ… FAISSç´¢å¼•çŠ¶æ€æ­£å¸¸")
        print(f"   ç´¢å¼•ä¸­çš„æ–‡æ¡£å—æ•°é‡: {total_docs}")
        print(f"   æ–‡æ¡£æ˜ å°„æ•°é‡: {len(knowledge_manager.doc_mapping)}")
        
        return True
    except Exception as e:
        print(f"âŒ å‘é‡ç´¢å¼•æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_knowledge_query():
    """æµ‹è¯•çŸ¥è¯†æŸ¥è¯¢"""
    print("\nğŸ” æµ‹è¯•3: çŸ¥è¯†æŸ¥è¯¢åŠŸèƒ½")
    try:
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            [{"keyword": "å¾®æ³¢é¥æ„Ÿ", "weight": 1.0}],
            [{"keyword": "åœŸå£¤æ¹¿åº¦", "weight": 1.0}],
            [{"keyword": "åå‘æ•£å°„ç³»æ•°", "weight": 1.0}],
            [{"keyword": "VHFé¢‘æ®µ", "weight": 1.0}],
            [{"keyword": "ä»‹ç”µå¸¸æ•°", "weight": 1.0}]
        ]
        
        for i, keywords in enumerate(test_queries):
            print(f"\n   æŸ¥è¯¢ {i+1}: {keywords[0]['keyword']}")
            result = knowledge_manager.query_knowledge(keywords, top_k=2)
            
            if result and len(result) > 100:
                print(f"   âœ… æŸ¥è¯¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
                print(f"   ğŸ“„ å†…å®¹é¢„è§ˆ: {result[:200]}...")
            else:
                print(f"   âš ï¸ æŸ¥è¯¢ç»“æœä¸ºç©ºæˆ–è¿‡çŸ­")
        
        return True
    except Exception as e:
        print(f"âŒ çŸ¥è¯†æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_document_addition():
    """æµ‹è¯•æ–‡æ¡£æ·»åŠ åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•4: æ–‡æ¡£æ·»åŠ åŠŸèƒ½")
    try:
        test_document = """
=== æµ‹è¯•æ–‡æ¡£ï¼šSARåœŸå£¤æ¹¿åº¦ç›‘æµ‹ ===

åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰åœ¨åœŸå£¤æ¹¿åº¦ç›‘æµ‹ä¸­çš„åº”ç”¨ï¼š

1. SARç³»ç»Ÿä¼˜åŠ¿ï¼š
   - é«˜ç©ºé—´åˆ†è¾¨ç‡ï¼ˆ1-100mï¼‰
   - å…¨å¤©å€™å·¥ä½œèƒ½åŠ›
   - å¤šæåŒ–è§‚æµ‹èƒ½åŠ›
   - ç©¿é€äº‘å±‚èƒ½åŠ›

2. åœŸå£¤æ¹¿åº¦åæ¼”åŸç†ï¼š
   - åŸºäºåå‘æ•£å°„æœºåˆ¶
   - åˆ©ç”¨ä»‹ç”µå¸¸æ•°å·®å¼‚
   - è€ƒè™‘åœ°è¡¨ç²—ç³™åº¦å½±å“
   - æ¤è¢«æ•ˆåº”æ ¡æ­£

3. å…³é”®æŠ€æœ¯å‚æ•°ï¼š
   - å·¥ä½œé¢‘æ®µï¼šLã€Cã€Xæ³¢æ®µ
   - æåŒ–æ–¹å¼ï¼šHHã€VVã€HVã€VH
   - å…¥å°„è§’ï¼š20-70åº¦
   - ç©ºé—´åˆ†è¾¨ç‡ï¼š1-100m

è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯ç³»ç»Ÿçš„æ–‡æ¡£æ·»åŠ å’Œæ£€ç´¢åŠŸèƒ½ã€‚
        """
        
        # è®°å½•æ·»åŠ å‰çš„æ–‡æ¡£æ•°é‡
        before_count = knowledge_manager.faiss_index.ntotal if knowledge_manager.faiss_index else 0
        
        # æ·»åŠ æµ‹è¯•æ–‡æ¡£
        success = add_document_to_knowledge_base(test_document, "test_sar_document.txt")
        
        if success:
            after_count = knowledge_manager.faiss_index.ntotal if knowledge_manager.faiss_index else 0
            print(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            print(f"   æ·»åŠ å‰æ–‡æ¡£å—æ•°é‡: {before_count}")
            print(f"   æ·»åŠ åæ–‡æ¡£å—æ•°é‡: {after_count}")
            print(f"   æ–°å¢æ–‡æ¡£å—æ•°é‡: {after_count - before_count}")
            
            # æµ‹è¯•æ–°æ·»åŠ æ–‡æ¡£çš„æŸ¥è¯¢
            print("\n   æµ‹è¯•æ–°æ·»åŠ æ–‡æ¡£çš„æŸ¥è¯¢:")
            keywords = [{"keyword": "SAR", "weight": 1.0}]
            result = knowledge_manager.query_knowledge(keywords, top_k=3)
            
            if "SAR" in result or "åˆæˆå­”å¾„é›·è¾¾" in result:
                print("   âœ… æ–°æ·»åŠ æ–‡æ¡£å¯ä»¥è¢«æˆåŠŸæ£€ç´¢")
            else:
                print("   âš ï¸ æ–°æ·»åŠ æ–‡æ¡£æ£€ç´¢ç»“æœä¸åŒ…å«é¢„æœŸå†…å®¹")
                
            return True
        else:
            print("âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ æ–‡æ¡£æ·»åŠ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_knowledge_sources():
    """æµ‹è¯•çŸ¥è¯†æºæ–‡ä»¶"""
    print("\nğŸ” æµ‹è¯•5: çŸ¥è¯†æºæ–‡ä»¶æ£€æŸ¥")
    try:
        sources_path = Path("file_storage/converted")
        
        if not sources_path.exists():
            print("âŒ çŸ¥è¯†æºç›®å½•ä¸å­˜åœ¨")
            return False
        
        txt_files = list(sources_path.glob("*.txt"))
        print(f"âœ… çŸ¥è¯†æºç›®å½•å­˜åœ¨")
        print(f"   æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶:")
        
        for file_path in txt_files:
            try:
                file_size = file_path.stat().st_size
                print(f"   ğŸ“„ {file_path.name} ({file_size} bytes)")
            except Exception as e:
                print(f"   âŒ è¯»å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path.name}: {str(e)}")
        
        return True
    except Exception as e:
        print(f"âŒ çŸ¥è¯†æºæ–‡ä»¶æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹RAGå‘é‡æ•°æ®åº“ç³»ç»Ÿæµ‹è¯•\n")
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_results.append(("åµŒå…¥æ¨¡å‹åŠ è½½", test_embedding_model()))
    test_results.append(("å‘é‡ç´¢å¼•çŠ¶æ€", test_vector_index()))
    test_results.append(("çŸ¥è¯†æºæ–‡ä»¶", test_knowledge_sources()))
    test_results.append(("çŸ¥è¯†æŸ¥è¯¢åŠŸèƒ½", test_knowledge_query()))
    test_results.append(("æ–‡æ¡£æ·»åŠ åŠŸèƒ½", test_document_addition()))
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*50)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:<20} {status}")
        if result:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RAGå‘é‡æ•°æ®åº“ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        print(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 